{
  "name": "Simple CSV to SQLite Ingestion",
  "nodes": [
    {
      "parameters": {
        "authentication": "none",
        "requestMethod": "GET",
        "url": "https://drive.usercontent.google.com/u/0/uc?id=1RXj_3txgmyX2Wyt9ZwM7l4axfi5A6EC-&export=download",
        "options": {}
      },
      "id": "http-request",
      "name": "Download CSV",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [
        480,
        300
      ],
      "continueOnFail": false
    },
    {
      "parameters": {
        "mode": "runOnceForAllItems",
        "pythonCode": "import sqlite3\nimport csv\nimport tempfile\nimport os\nimport base64\nfrom datetime import datetime\n\n# Get the downloaded file data\nbinary_data = items[0]['binary']['data']\n\n# Convert to string and decode base64\nif hasattr(binary_data, 'toString'):\n    binary_data = binary_data.toString()\nelse:\n    binary_data = str(binary_data)\n\nbinary_data = base64.b64decode(binary_data)\n\n# Create temporary CSV file\nwith tempfile.NamedTemporaryFile(mode='w+b', suffix='.csv', delete=False) as temp_file:\n    temp_file.write(binary_data)\n    csv_file_path = temp_file.name\n\n# Use ABSOLUTE path that definitely works\ndb_path = '/tmp/simple_data_warehouse.sqlite'\nprint(f\"Database path: {db_path}\")\n\ntry:\n    # Connect to SQLite\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n    \n    # Read CSV headers\n    with open(csv_file_path, 'r', encoding='utf-8') as csv_file:\n        csv_reader = csv.reader(csv_file)\n        headers = next(csv_reader)\n    \n    # Drop and recreate table to ensure clean state\n    cursor.execute('DROP TABLE IF EXISTS csv_data')\n    \n    # Create simple table\n    create_sql = 'CREATE TABLE csv_data ('\n    create_sql += 'id INTEGER PRIMARY KEY AUTOINCREMENT, '\n    create_sql += ', '.join([f'\\\"{col}\\\" TEXT' for col in headers])\n    create_sql += ', load_timestamp TEXT, source_file TEXT)'\n    \n    cursor.execute(create_sql)\n    \n    # Insert data\n    with open(csv_file_path, 'r', encoding='utf-8') as csv_file:\n        csv_reader = csv.DictReader(csv_file)\n        \n        row_count = 0\n        for row in csv_reader:\n            columns = list(row.keys())\n            values = list(row.values())\n            \n            # Add metadata\n            columns.extend(['load_timestamp', 'source_file'])\n            values.extend([datetime.now().isoformat(), 'ads_spend_data.csv'])\n            \n            placeholders = ', '.join(['?'] * len(columns))\n            column_names = ', '.join([f'\\\"{col}\\\"' for col in columns])\n            \n            cursor.execute(f'INSERT INTO csv_data ({column_names}) VALUES ({placeholders})', values)\n            row_count += 1\n    \n    conn.commit()\n    conn.close()\n    \n    print(f'Success! Loaded {row_count} rows')\n    \n    output = [{\n        'json': {\n            'message': f'Loaded {row_count} rows successfully',\n            'rows_loaded': row_count,\n            'database_path': db_path,\n            'status': 'success'\n        }\n    }]\n\nexcept Exception as e:\n    print(f'Error: {e}')\n    output = [{\n        'json': {\n            'error': str(e),\n            'status': 'failed'\n        }\n    }]\n    raise e\n\nfinally:\n    if os.path.exists(csv_file_path):\n        os.unlink(csv_file_path)\n\nreturn output"
      },
      "id": "ingest-sqlite",
      "name": "Ingest to SQLite",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        680,
        300
      ],
      "continueOnFail": false
    },
    {
      "parameters": {
        "mode": "runOnceForAllItems",
        "pythonCode": "import sqlite3\n\n# Use the same database path\ndb_path = '/tmp/simple_data_warehouse.sqlite'\n\ntry:\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n    \n    # Count rows\n    cursor.execute('SELECT COUNT(*) FROM csv_data')\n    row_count = cursor.fetchone()[0]\n    \n    # Get sample data\n    cursor.execute('SELECT * FROM csv_data LIMIT 2')\n    sample_rows = cursor.fetchall()\n    \n    # Get column names\n    cursor.execute('PRAGMA table_info(csv_data)')\n    columns = [col[1] for col in cursor.fetchall()]\n    \n    conn.close()\n    \n    print(f'Total rows: {row_count}')\n    print(f'Columns: {columns}')\n    print('Sample rows:', sample_rows[:2])\n    \n    output = [{\n        'json': {\n            'total_rows': row_count,\n            'columns': columns,\n            'sample_data': str(sample_rows[:2]),\n            'verification': 'success'\n        }\n    }]\n    \nexcept Exception as e:\n    print(f'Verification error: {e}')\n    output = [{\n        'json': {\n            'error': str(e),\n            'verification': 'failed'\n        }\n    }]\n\nreturn output"
      },
      "id": "verify-data",
      "name": "Verify Data",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        880,
        300
      ],
      "continueOnFail": false
    }
  ],
  "connections": {
    "Download CSV": {
      "main": [
        [
          {
            "node": "Ingest to SQLite",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Ingest to SQLite": {
      "main": [
        [
          {
            "node": "Verify Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "settings": {
    "executionOrder": "v1"
  },
  "staticData": null,
  "pinData": null,
  "versionId": "1",
  "triggerCount": 1
}